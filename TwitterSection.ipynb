{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-22050e3ad4b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time, sleep\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NameOfCompanies = [\"Apple\", \"Samsung\", \"Huawei\"]\n",
    "companies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['Apple since:2019-11-01 until:2019-11-05', 'Apple since:2019-11-05 until:2019-11-10', 'Apple since:2019-11-10 until:2019-11-14', 'Apple since:2019-11-14 until:2019-11-19', 'Apple since:2019-11-19 until:2019-11-24', 'Apple since:2019-11-24 until:2019-11-28', 'Apple since:2019-11-28 until:2019-12-03', 'Apple since:2019-12-03 until:2019-12-07', 'Apple since:2019-12-07 until:2019-12-12', 'Apple since:2019-12-12 until:2019-12-17', 'Apple since:2019-12-17 until:2019-12-21', 'Apple since:2019-12-21 until:2019-12-26', 'Apple since:2019-12-26 until:2019-12-30', 'Apple since:2019-12-30 until:2020-01-04', 'Apple since:2020-01-04 until:2020-01-09', 'Apple since:2020-01-09 until:2020-01-13', 'Apple since:2020-01-13 until:2020-01-18', 'Apple since:2020-01-18 until:2020-01-22', 'Apple since:2020-01-22 until:2020-01-27', 'Apple since:2020-01-27 until:2020-02-01']\n",
      "INFO: Got 19 tweets (19 new).\n",
      "INFO: Got 37 tweets (18 new).\n",
      "INFO: Got 56 tweets (19 new).\n",
      "INFO: Got 76 tweets (20 new).\n",
      "INFO: Got 94 tweets (18 new).\n",
      "INFO: Got 112 tweets (18 new).\n",
      "INFO: Got 131 tweets (19 new).\n",
      "INFO: Got 148 tweets (17 new).\n",
      "INFO: Got 168 tweets (20 new).\n",
      "INFO: Got 185 tweets (17 new).\n",
      "INFO: Got 203 tweets (18 new).\n",
      "INFO: Got 223 tweets (20 new).\n",
      "INFO: Got 243 tweets (20 new).\n",
      "INFO: Got 262 tweets (19 new).\n",
      "INFO: Got 282 tweets (20 new).\n",
      "INFO: Got 301 tweets (19 new).\n",
      "INFO: Got 319 tweets (18 new).\n",
      "INFO: Got 339 tweets (20 new).\n",
      "INFO: Got 359 tweets (20 new).\n",
      "INFO: Got 376 tweets (17 new).\n",
      "INFO: queries: ['Samsung since:2019-11-01 until:2019-11-05', 'Samsung since:2019-11-05 until:2019-11-10', 'Samsung since:2019-11-10 until:2019-11-14', 'Samsung since:2019-11-14 until:2019-11-19', 'Samsung since:2019-11-19 until:2019-11-24', 'Samsung since:2019-11-24 until:2019-11-28', 'Samsung since:2019-11-28 until:2019-12-03', 'Samsung since:2019-12-03 until:2019-12-07', 'Samsung since:2019-12-07 until:2019-12-12', 'Samsung since:2019-12-12 until:2019-12-17', 'Samsung since:2019-12-17 until:2019-12-21', 'Samsung since:2019-12-21 until:2019-12-26', 'Samsung since:2019-12-26 until:2019-12-30', 'Samsung since:2019-12-30 until:2020-01-04', 'Samsung since:2020-01-04 until:2020-01-09', 'Samsung since:2020-01-09 until:2020-01-13', 'Samsung since:2020-01-13 until:2020-01-18', 'Samsung since:2020-01-18 until:2020-01-22', 'Samsung since:2020-01-22 until:2020-01-27', 'Samsung since:2020-01-27 until:2020-02-01']\n",
      "INFO: Got 20 tweets (20 new).\n",
      "INFO: Got 40 tweets (20 new).\n",
      "INFO: Got 59 tweets (19 new).\n",
      "INFO: Got 79 tweets (20 new).\n",
      "INFO: Got 99 tweets (20 new).\n",
      "INFO: Got 118 tweets (19 new).\n",
      "INFO: Got 138 tweets (20 new).\n",
      "INFO: Got 157 tweets (19 new).\n",
      "INFO: Got 176 tweets (19 new).\n",
      "INFO: Got 195 tweets (19 new).\n",
      "INFO: Got 215 tweets (20 new).\n",
      "INFO: Got 234 tweets (19 new).\n",
      "INFO: Got 254 tweets (20 new).\n",
      "INFO: Got 273 tweets (19 new).\n",
      "INFO: Got 292 tweets (19 new).\n",
      "INFO: Got 309 tweets (17 new).\n",
      "INFO: Got 329 tweets (20 new).\n",
      "INFO: Got 349 tweets (20 new).\n",
      "INFO: Got 369 tweets (20 new).\n",
      "INFO: Got 388 tweets (19 new).\n",
      "INFO: queries: ['Huawei since:2019-11-01 until:2019-11-05', 'Huawei since:2019-11-05 until:2019-11-10', 'Huawei since:2019-11-10 until:2019-11-14', 'Huawei since:2019-11-14 until:2019-11-19', 'Huawei since:2019-11-19 until:2019-11-24', 'Huawei since:2019-11-24 until:2019-11-28', 'Huawei since:2019-11-28 until:2019-12-03', 'Huawei since:2019-12-03 until:2019-12-07', 'Huawei since:2019-12-07 until:2019-12-12', 'Huawei since:2019-12-12 until:2019-12-17', 'Huawei since:2019-12-17 until:2019-12-21', 'Huawei since:2019-12-21 until:2019-12-26', 'Huawei since:2019-12-26 until:2019-12-30', 'Huawei since:2019-12-30 until:2020-01-04', 'Huawei since:2020-01-04 until:2020-01-09', 'Huawei since:2020-01-09 until:2020-01-13', 'Huawei since:2020-01-13 until:2020-01-18', 'Huawei since:2020-01-18 until:2020-01-22', 'Huawei since:2020-01-22 until:2020-01-27', 'Huawei since:2020-01-27 until:2020-02-01']\n",
      "INFO: Got 20 tweets (20 new).\n",
      "INFO: Got 39 tweets (19 new).\n",
      "INFO: Got 59 tweets (20 new).\n",
      "INFO: Got 78 tweets (19 new).\n",
      "INFO: Got 96 tweets (18 new).\n",
      "INFO: Got 116 tweets (20 new).\n",
      "INFO: Got 135 tweets (19 new).\n",
      "INFO: Got 154 tweets (19 new).\n",
      "INFO: Got 173 tweets (19 new).\n",
      "INFO: Got 193 tweets (20 new).\n",
      "INFO: Got 213 tweets (20 new).\n",
      "INFO: Got 230 tweets (17 new).\n",
      "INFO: Got 250 tweets (20 new).\n",
      "INFO: Got 270 tweets (20 new).\n",
      "INFO: Got 290 tweets (20 new).\n",
      "INFO: Got 310 tweets (20 new).\n",
      "INFO: Got 330 tweets (20 new).\n",
      "INFO: Got 349 tweets (19 new).\n",
      "INFO: Got 369 tweets (20 new).\n",
      "INFO: Got 388 tweets (19 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Samsung\n",
      "Huawei\n",
      "Finally finished!\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i < len(NameOfCompanies):   \n",
    "    begin_date = dt.date(2019,11,1)\n",
    "    end_date = dt.date(2020,2,1)\n",
    "    limit = 0\n",
    "    lang = \"english\"\n",
    "    CompanyTweets = query_tweets(NameOfCompanies[i], limit=limit, begindate=begin_date, enddate=end_date, lang=lang)\n",
    "    companies.append(CompanyTweets)\n",
    "    i= i+1\n",
    "else:\n",
    "    print(NameOfCompanies[0])\n",
    "    print(NameOfCompanies[1])\n",
    "    print(NameOfCompanies[2])\n",
    "    print(\"Finally finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNames = ['AppleTweets','SamsungTweets','HuaweiTweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for company in companies:\n",
    "    dfNames[i] = pd.DataFrame(t.__dict__ for t in  company)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AppleTweets = dfNames[0]\n",
    "SamsungTweets = dfNames[1]\n",
    "HuaweiTweets = dfNames[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376, 21)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AppleTweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 21)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SamsungTweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 21)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HuaweiTweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AppleTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SamsungTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuaweiTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AppleTweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SamsungTweetssungTweetsmsungTweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuaweiTweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AppleTweets.drop([\"screen_name\", \"username\",\"user_id\",\"tweet_id\",\"tweet_url\",\"timestamp_epochs\",\n",
    "                 \"text_html\",\"has_media\",\"is_replied\",\"is_reply_to\",\"parent_tweet_id\",\"reply_to_users\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AppleTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SamsungTweets.drop([\"screen_name\", \"username\",\"user_id\",\"tweet_id\",\"tweet_url\",\"timestamp_epochs\",\n",
    "                 \"text_html\",\"has_media\",\"is_replied\",\"is_reply_to\",\"parent_tweet_id\",\"reply_to_users\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SamsungTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuaweiTweets.drop([\"screen_name\", \"username\",\"user_id\",\"tweet_id\",\"tweet_url\",\"timestamp_epochs\",\n",
    "                 \"text_html\",\"has_media\",\"is_replied\",\"is_reply_to\",\"parent_tweet_id\",\"reply_to_users\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuaweiTweets.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization with POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    # map POS tag to the first character that lemmatize() accepts\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# clean and tokenize the text\n",
    "def tokenize(text, stem=True, lemmatize=True, min_char=3, max_char=30):\n",
    "    # tokenize, remove punctuations, digits  and make lowercase\n",
    "    remove_map = dict((ord(char), ' ') for char in (string.digits+string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text.lower().translate(remove_map))\n",
    "    tokens = [item for item in tokens if ((len(item)>=min_char)&(len(item)<=max_char))]\n",
    "\n",
    "    # lemmatization\n",
    "    if lemmatize:\n",
    "        tokens = [WordNetLemmatizer().lemmatize(item, get_wordnet_pos(item)) for item in tokens]\n",
    "    # stemming\n",
    "    if stem:\n",
    "        tokens = [PorterStemmer().stem(item) for item in tokens]\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "# vectorize text to either tfidf or bow\n",
    "def vectorize(doc_list, method='tfidf', stem=True, lemmatize=True, **kwargs):\n",
    "    # words that don't have much value and should not be included in the vectorization\n",
    "    Additional_stop_words = ['will', 'laughter', 'applause', 'now']\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(Additional_stop_words)\n",
    "    \n",
    "    if method=='tfidf':\n",
    "        vectorizer = TfidfVectorizer(tokenizer=lambda text: tokenize(text, stem=stem, lemmatize=lemmatize),\n",
    "                                     stop_words=stop_words, **kwargs)\n",
    "    \n",
    "    # for simplicity I assume if not \"tfidf\", method is \"bow\" or \"tf\"\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(tokenizer=lambda text: tokenize(text, stem=stem, lemmatize=lemmatize),\n",
    "                                     stop_words=stop_words, **kwargs)\n",
    "    \n",
    "    vect = vectorizer.fit_transform(doc_list)\n",
    "    \n",
    "    return vect, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lenght of tweets given by each user for each company\n",
    "def plot_word_counts(df, category='category', title=None, figsize=(12,6), suptitle_y=0.95): \n",
    "    fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "    plt.suptitle(title, fontsize=20, y=suptitle_y)\n",
    "\n",
    "    # dictionary to map colors to each company\n",
    "    my_palette = {'Apple': 'white', 'Samsung': 'blue','Huawei' : 'red'} \n",
    "\n",
    "    # violin plot to show the distribution of the word count\n",
    "    sns.violinplot(x=\"word_count\", y=category, hue=\"company\",\n",
    "                   data=df, split=True, palette=my_palette, ax=axs[0])\n",
    "    axs[0].legend().set_visible(False)\n",
    "    axs[0].set_ylabel('')\n",
    "\n",
    "    # bar plot to show the average word count\n",
    "    sns.barplot(x=\"word_count\", y=category, hue=\"president\",\n",
    "                   data=df, palette=my_palette, ax=axs[1])\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_ylabel('')\n",
    "    axs[1].set_xlabel('mean_word_count')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word cloud of a given text\n",
    "def plot_word_cloud(words, colormap='bwr', title=None, width=1000, height=800):\n",
    "    # set the stop words\n",
    "    stopwords = set(STOPWORDS)\n",
    "    Additional_stop_words = ['will', 'laughter', 'applause', 'now']\n",
    "    for s in  Additional_stop_words:\n",
    "        stopwords.add(s)\n",
    "\n",
    "    # generate WordCloud\n",
    "    if type(words)==str:\n",
    "        wc = WordCloud(background_color=\"white\", stopwords=stopwords, \n",
    "                       width=width, height=height, colormap=colormap).generate(words)\n",
    "    # for simplicity, I assume that if words is not an string, it is a dictionary of word frequencies\n",
    "    else:\n",
    "        wc = WordCloud(background_color=\"white\", stopwords=stopwords, \n",
    "                       width=width, height=height, colormap=colormap).generate_from_frequencies(words)\n",
    "\n",
    "    # show the WordCloud\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=25, y=1.01)\n",
    "\n",
    "\n",
    "# plot two word cloud, one for each president\n",
    "def plot_two_word_clouds(df, title=None, method='str',**kwargs):\n",
    "    my_color_map = ['Blues', 'Reds']\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, p in enumerate(['Barack Obama', 'George W. Bush']):\n",
    "        if method=='str':\n",
    "            words = (' '.join(df.loc[df['president']==p, 'content'])).lower()\n",
    "        # for simplicity, I assume that if not string, the method is word frequency\n",
    "        else:\n",
    "            words = Counter(df.loc[df['president']==p, 'entity'])\n",
    "            \n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plot_word_cloud(words, my_color_map[i], title=p, **kwargs)\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=35, y=0.9)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot two separate pie cahrts of sentiment analysis results, one for each president\n",
    "def plot_sentiment_pie(df, sent_label, title=None):\n",
    "    counts = 100*(df.groupby([sent_label, 'president'])[sent_label].count() /\n",
    "                  df.groupby(['president'])[sent_label].count())\n",
    "    xs = np.array(counts).reshape(3, 2)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, p in enumerate(['Barack Obama', 'George W. Bush']):    \n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.pie(xs[:,i], explode=[0.07, 0.07, 0.07], labels=['Negative', 'Neutral', 'Positive'],\n",
    "                colors=['#FD5D5F', '#FEDD64', '#38BF6F'], autopct='%.1f%%', shadow=True, textprops={'fontsize': 15})\n",
    "        plt.axis('equal')\n",
    "        plt.title(p, fontsize=20, y=0.94)\n",
    "        \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=25, y=0.96)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sentiment bar charts across all categories\n",
    "def plot_sentiment_bars(df, sent_label, category, title=None):\n",
    "    counts = 100*(df.groupby([sent_label, category])[sent_label].count() /\n",
    "                  df.groupby([category])[sent_label].count())\n",
    "    y = np.sort(df[category].unique())\n",
    "    xs = np.array(counts).reshape(3, len(y))\n",
    "    \n",
    "    # positive bar\n",
    "    p1 = plt.barh(y, xs[2], color='#38BF6F')\n",
    "    # negative bar\n",
    "    p2 = plt.barh(y, xs[0], left=xs[2], color='#FD5D5F')\n",
    "    # neutral bar\n",
    "    p3 = plt.barh(y, xs[1], left=xs[2]+xs[0], color='#FEDD64')\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Percentage of all sentences', fontsize=15)\n",
    "    plt.legend((p1, p2, p3), ('Positive', 'Negative', 'Neutral'))\n",
    "    if title:\n",
    "        plt.title(title, fontsize=20)\n",
    "\n",
    "\n",
    "# plot two separate sentiment bar charts, one for each president\n",
    "def plot_two_sentiment_bars(df, sent_label, title=None, category='category_subcategory'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, p in enumerate(['Barack Obama', 'George W. Bush']):    \n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plot_sentiment_bars(df.loc[df['president']==p], sent_label, category=category, title=p)\n",
    "        if i==0:\n",
    "            plt.legend().set_visible(False)\n",
    "    \n",
    "    plt.yticks([])\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=30, y=1)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ease of presentation, make a new column \"category: subcategory\"\n",
    "data['category_subcategory'] = ['%s: %s' %(cat, subcat) for cat, subcat in\n",
    "                                zip(data['category'].values, data['subcategory'].values)]\n",
    "\n",
    "# count the number of words in each doc\n",
    "word_count_list = np.zeros(data.shape[0], dtype=np.int)\n",
    "doc_list = data['content'].values\n",
    "for i in data.index:\n",
    "    word_count_list[i] = len(doc_list[i].split())\n",
    "\n",
    "# add the word count as a new column\n",
    "data['word_count'] = word_count_list\n",
    "\n",
    "# keep the memory clean\n",
    "del word_count_list, doc_list\n",
    "    \n",
    "# check the df\n",
    "print(data.shape)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1be231bdb2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_two_word_clouds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAppleTweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Word Cloud of All Apple'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-289650fdfd73>\u001b[0m in \u001b[0;36mplot_two_word_clouds\u001b[1;34m(df, title, method, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_two_word_clouds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'str'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmy_color_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Blues'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Reds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Barack Obama'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'George W. Bush'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'str'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plot_two_word_clouds(AppleTweets, 'Word Cloud of All Apple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_word_clouds(SamungTweets, 'Word Cloud of Samsung')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_word_clouds(HuaweiTweets, 'Word Cloud of Huawei')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Named Entity Recognition\n",
    "# Here comes our first bet. We can use NER to extract the named entities mentioned by each speacker. This could give us a better picture as to what they talked about the most.\n",
    "\n",
    "# I am going to concentrate on 3 main categories of named entities, location, person and organisation.\n",
    "\n",
    "# I am using a pretrained model from SpaCy. After doing some research on different pretrained NER options and their differences, I decided to use spaCy. It has a very good performance, even more than what we really need here. However, given more time, one could try to find / build an even more accurate NER model.\n",
    "\n",
    "# Let's dive into NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a few hours to run, I really suggest to load the saved results instead\n",
    "if user_confirmation():\n",
    "    t = time()\n",
    "    \n",
    "    # make a new df to store NER results\n",
    "    columns=['sent_index', 'entity', 'label']\n",
    "    sent_entities = pd.DataFrame(columns=columns)\n",
    "    # convert df to array to improve loop time performance\n",
    "    sentence_list = sent_data['sentence'].values\n",
    "    # for every sentence, run NER and store the result\n",
    "    for i in sent_data.index:\n",
    "        sent = nlp(sentence_list[i])\n",
    "        # to improve loop time performance, convert integer to string\n",
    "        temp = [[str(i), x.text, x.label_] for x in sent.ents]\n",
    "        # add 1 row for each NER result\n",
    "        sent_entities = sent_entities.append(pd.DataFrame(temp, columns=columns), ignore_index=True)\n",
    "\n",
    "    # convert sent_index back to integer\n",
    "    sent_entities['sent_index'] = sent_entities['sent_index'].astype(np.int)\n",
    "\n",
    "    print('Done in %i Seconds' %(time()-t))\n",
    "    sent_entities.to_csv('data/sent_entities.csv', index=False)\n",
    "\n",
    "    # keep the memory clean\n",
    "    del sentence_list\n",
    "else:\n",
    "    t = time()\n",
    "    # since it takes time to generate this, we load it from here\n",
    "    sent_entities = pd.read_csv('data/sent_entities.csv')\n",
    "    print('Done in %i Seconds' %(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other columns to sent_entities\n",
    "sent_entities['president'] = sent_data.loc[sent_entities['sent_index'], ['president']].values\n",
    "\n",
    "# check the df\n",
    "print(sent_entities.shape)\n",
    "sent_entities.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
